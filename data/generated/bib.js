define({ entries : {
    "GaoLin2024RLDo": {
        "abstract": "Neural implicit representations, including Neural Distance Fields and Neural Radiance Fields, have demonstrated significant capabilities for reconstructing surfaces with complicated geometry and topology, and generating novel views of a scene. Nevertheless, it is challenging for users to directly deform or manipulate these implicit representations with large deformations in a real-time fashion. Gaussian Splatting (GS) has recently become a promising method with explicit geometry for representing static scenes and facilitating high-quality and real-time synthesis of novel views. However, it cannot be easily deformed due to the use of discrete Gaussians and the lack of explicit topology. To address this, we develop a novel GS-based method (GaussianMesh) that enables interactive deformation. Our key idea is to design an innovative mesh-based GS representation, which is integrated into Gaussian learning and manipulation. 3D Gaussians are defined over an explicit mesh, and they are bound with each other: the rendering of 3D Gaussians guides the mesh face split for adaptive refinement, and the mesh face split directs the splitting of 3D Gaussians. Moreover, the explicit mesh constraints help regularize the Gaussian distribution, suppressing poor-quality Gaussians (e.g., misaligned Gaussians, long-narrow shaped Gaussians), thus enhancing visual quality and reducing artifacts during deformation. Based on this representation, we further introduce a large-scale Gaussian deformation technique to enable deformable GS, which alters the parameters of 3D Gaussians according to the manipulation of the associated mesh. Our method benefits from existing mesh deformation datasets for more realistic data-driven Gaussian deformation. Extensive experiments show that our approach achieves high-quality reconstruction and effective deformation, while maintaining the promising rendering results at a high frame rate (65 FPS on average on a single commodity GPU).",
        "address": "New York, NY, USA",
        "author": "Gao, Lin and Yang, Jie and Zhang, Bo-Tao and Sun, Jia-Mu and Yuan, Yu-Jie and Fu, Hongbo and Lai, Yu-Kun",
        "copyright": "Copyright is held by the owner/author(s). Publication rights licensed to ACM.",
        "doi": "10.1145/3687756",
        "issn": "0730-0301",
        "journal": "ACM transactions on graphics",
        "keywords": "type:Large-Scale Scene Reconstruction, topics:Computer graphics, Computing methodologies , Image manipulation , Image-based rendering , Mesh models , Shape modeling",
        "language": "eng",
        "number": "6",
        "pages": "1-17",
        "publisher": "ACM",
        "series": "ACM transactions on graphics",
        "title": "Real-time Large-scale Deformation of Gaussian Splatting",
        "type": "article",
        "url": "https://dl-acm-org.nottingham.idm.oclc.org/doi/abs/10.1145/3687756",
        "volume": "43",
        "year": "2024"
    },
    "JangMinHyuk2025Htca": {
        "abstract": "The growth of the augmented reality industry has increased demand for three-dimensional (3D) cartoon avatars, requiring expertise from computer graphics designers. Recent 3D Gaussian splatting methods have successfully reconstructed 3D avatars from videos, establishing them as a promising solution for this task. However, these methods primarily focus on real-world videos, limiting their effectiveness in the cartoon domain. In this paper, we present an artificial intelligence (AI)-based method for 3D avatar reconstruction from animated cartoon videos, addressing the physically unrealistic and unstructured geometries of cartoons, as well as the varying texture styles across frames. Our surface fitting module models the unstructured geometry of cartoon characters by integrating the surfaces observed from multiple views into a 3D avatar. We design a style normalizer that adjusts color distributions to reduce texture color inconsistencies in each frame of animated cartoons. Additionally, to better capture the simplified color distributions of cartoons, we design a frequency transform loss that focuses on low-frequency components. Our method significantly outperforms state-of-the-art methods, achieving approximately a 25% improvement in Learned Perceptual Image Patch Similarity (LPIPS) with a score of 0.052 over baselines across the Cartoon Neuman and ToonVid datasets, which comprise 10 videos with diverse styles and poses. Consequently, this paper presents a promising solution to meet the growing demand for high-quality 3D cartoon avatar modeling. [Display omitted] \u2022Reconstruct 3D cartoon avatars from a single animated video.\u2022Surface fitting captures challenging geometry of cartoon avatars.\u2022Style normalizer trains color distribution; frequency loss sharpens fine details.\u2022Novel pixel-wise segmentation module separates characters from backgrounds.\u2022Outperform state-of-the-art methods, meeting the demand for 3D character modeling.",
        "author": "Jang, MinHyuk and Kim, Jong Wook and Jang, Youngdong and Kim, Donghyun and Roh, Wonseok and Hwang, InYong and Lin, Guang and Kim, Sangpil",
        "copyright": "2025 Elsevier Ltd",
        "doi": "10.1016/j.engappai.2025.110305",
        "issn": "0952-1976",
        "journal": "Engineering applications of artificial intelligence",
        "keywords": "type:Human Avatar Reconstruction, topics: Artificial intelligence , Gaussian splatting , Metaverse , Pixel-wise segmentation , Three-dimensional avatar reconstruction",
        "language": "eng",
        "pages": "110305-",
        "publisher": "Elsevier Ltd",
        "series": "Engineering applications of artificial intelligence",
        "title": "High-quality three-dimensional cartoon avatar reconstruction with Gaussian splatting",
        "type": "article",
        "url": "https://pdf.sciencedirectassets.com/271095/1-s2.0-S0952197625X00075/1-s2.0-S0952197625003057/main.pdf?X-Amz-Security-Token",
        "volume": "148",
        "year": "2025"
    },
    "KerblBernhard20233GSf": {
        "abstract": "Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (\u2265 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.",
        "address": "New York, NY, USA",
        "author": "Kerbl, Bernhard and Kopanas, Georgios and Leimkuehler, Thomas and Drettakis, George",
        "copyright": "ACM",
        "doi": "10.1145/3592433",
        "issn": "0730-0301",
        "journal": "ACM transactions on graphics",
        "keywords": "type:Survey Paper and Foundational Work, topics:Rendering, Computer graphics, Computer Science, Computing methodologies, Machine learning, Machine learning, topics:Point-based models, Rasterization, Shape modeling",
        "language": "eng",
        "number": "4",
        "pages": "1-14",
        "publisher": "ACM",
        "series": "ACM",
        "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
        "type": "article",
        "url": "https://dl-acm-org.nottingham.idm.oclc.org/doi/pdf/10.1145/3592433",
        "volume": "42",
        "year": "2023"
    },
    "LiWan2025FFRa": {
        "abstract": "Dynamic reconstruction technology presents significant promise for applications in visual and interactive fields. Current techniques utilizing 3D Gaussian Splatting show favorable results and fast reconstruction speed. However, as scene expanding, using individual Gaussian structure (i) leads to instability in large-scale dynamic reconstruction, marked by abrupt deformation, and (ii) the heuristic densification of individuals suffers significant redundancy. Tackling these issues, we propose a jointed Gaussian representation method named FRPGS, which learns the global information and the deformation using center Gaussians and generates the neural Gaussians around them for local detail. Specifically, FRPGS employs center Gaussians initialized from point clouds, which are learned with a deformation field for representing global relationships and dynamic motion over time. Then, for each center Gaussian, attribute networks generate neural Gaussians that move under the linked center Gaussian driving, thereby ensuring structural integrity during movement within this joint-based representation. Finally, to reduce Gaussian redundancy, a densification strategy is developed based on the average cumulative gradient of the associated neural Gaussians, imposing strict limits on the growing of center Gaussians without compromising accuracy. Additionally, we established a large-scale dynamic indoor dataset at the MuLong Laboratory of ZTE Corporation. Evaluations demonstrate that FRPGS significantly outperforms state-of-the-art methods in both training efficiency and reconstruction quality, achieving over a 50% (up to 74%) improvement in efficiency on an RTX 4090. FRPGS also supports the 4K resolution reconstruction of 60 frames simultaneously.",
        "author": "Li, Wan and Pan, Xiao and Lin, Jiaxin and Lu, Ping and Feng, Daquan and Shi, Wenzhe",
        "doi": "10.1109/TCSVT.2025.3557012",
        "issn": "1051-8215",
        "journal": "IEEE transactions on circuits and systems for video technology",
        "keywords": "type:Monocular Dynamic Scene, topics: Dynamic Reconstruction , Adaptation models , Deformation , Dynamics , Gaussians Splatting , Kernel , Large-scale Indoor scene , Neural radiance field , Novel View Synthesis , Point cloud compression , Rendering (computer graphics) , Three-dimensional displays , Training , Visualization",
        "language": "eng",
        "pages": "1-1",
        "publisher": "IEEE",
        "series": "IEEE transactions on circuits and systems for video technology",
        "title": "FRPGS: Fast, Robust, and Photorealistic Monocular Dynamic Scene Reconstruction with Deformable 3D Gaussians",
        "type": "article",
        "url": "https://ieeexplore-ieee-org.nottingham.idm.oclc.org/stamp/stamp.jsp?tp",
        "year": "2025"
    },
    "LyuXiaoyang20243ISR": {
        "abstract": "In this paper, we present an implicit surface reconstruction method with 3D Gaussian Splatting (3DGS), namely 3DGSR, that allows for accurate 3D reconstruction with intricate details while inheriting the high efficiency and rendering quality of 3DGS. The key insight is to incorporate an implicit signed distance field (SDF) within 3D Gaussians for surface modeling, and to enable the alignment and joint optimization of both SDF and 3D Gaussians. To achieve this, we design coupling strategies that align and associate the SDF with 3D Gaussians, allowing for unified optimization and enforcing surface constraints on the 3D Gaussians. With alignment, optimizing the 3D Gaussians provides supervisory signals for SDF learning, enabling the reconstruction of intricate details. However, this only offers sparse supervisory signals to the SDF at locations occupied by Gaussians, which is insufficient for learning a continuous SDF. Then, to address this limitation, we incorporate volumetric rendering and align the rendered geometric attributes (depth, normal) with that derived from 3DGS. In sum, these two designs allow SDF and 3DGS to be aligned, jointly optimized, and mutually boosted. Our extensive experimental results demonstrate that our 3DGSR enables high-quality 3D surface reconstruction while preserving the efficiency and rendering quality of 3DGS. Besides, our method competes favorably with leading surface reconstruction techniques while offering a more efficient learning process and much better rendering qualities.",
        "address": "New York, NY, USA",
        "author": "Lyu, Xiaoyang and Sun, Yang-Tian and Huang, Yi-Hua and Wu, Xiuzhe and Yang, Ziyi and Chen, Yilun and Pang, Jiangmiao and Qi, Xiaojuan",
        "copyright": "Copyright is held by the owner/author(s). Publication rights licensed to ACM.",
        "doi": "10.1145/3687952",
        "issn": "0730-0301",
        "journal": "ACM transactions on graphics",
        "keywords": "type:Large-Scale Scene Reconstruction, topics:Computer graphics , Computing methodologies , Mesh geometry models , Shape modeling",
        "language": "eng",
        "number": "6",
        "pages": "1-12",
        "publisher": "ACM",
        "series": "ACM transactions on graphics",
        "title": "3DGSR: Implicit Surface Reconstruction with 3D Gaussian Splatting",
        "type": "article",
        "url": "https://dl-acm-org.nottingham.idm.oclc.org/doi/pdf/10.1145/3687952",
        "volume": "43",
        "year": "2024"
    },
    "QianJiating2025CC3G": {
        "abstract": "Methods based on 3D Gaussian Splatting (3DGS) for surface reconstruction face challenges when applied to large-scale scenes captured by UAV. Because the number of 3D Gaussians increases dramatically, leading to significant computational requirement and limiting the fineness of surface reconstruction. To address this challenge, we propose C3DGS that compresses 3D Gaussian model and ensures the quality of surface reconstruction of large-scale scenes in the face of heavy computational costs. Our method quantifies the contribution of 3D Gaussians to the surface reconstruction and prunes redundant 3D Gaussians to reduce the computational requirement of the model. In addition, pruning 3D Gaussians inevitably incurs loss, and in order to guarantee as many details as possible in the surface reconstruction of a complex scene, we use a ray tracing volume rendering method that can better evaluate the opacity of 3D Gaussians. Furthermore, we introduce two regularization terms to enhance the geometric consistency of multiple views, thus improving the realism of surface reconstruction. Experiments show that our method outperforms other 3DGS-based surface reconstruction methods when facing large-scale scenes.",
        "address": "Piscataway",
        "author": "Qian, Jiating and Yan, Yiming and Gao, Fengjiao and Ge, Baoyu and Wei, Maosheng and Shangguan, Boyi and He, Guangjun",
        "copyright": "Copyright The Institute of Electrical and Electronics Engineers, Inc. (IEEE) 2025",
        "doi": "10.1109/JSTARS.2025.3529261",
        "issn": "1939-1404",
        "journal": "IEEE journal of selected topics in applied earth observations and remote sensing",
        "keywords": "type:Large-Scale Scene Reconstruction, topics: 3D Gaussian splatting (3DGS) , Autonomous aerial vehicles , Cameras , Computational modeling , Computer applications , Computing costs , Fineness , Image compression , Image reconstruction , large-scale scenes , multiview UAV images , Opacity , Point cloud compression , Ray tracing , Regularization , Rendering (computer graphics) , Solid modeling , Surface reconstruction , Three-dimensional displays , Training",
        "language": "eng",
        "pages": "4396-4409",
        "publisher": "IEEE",
        "series": "IEEE journal of selected topics in applied earth observations and remote sensing",
        "title": "C3DGS: Compressing 3D Gaussian Model for Surface Reconstruction of Large-Scale Scenes Based on Multiview UAV Images",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp",
        "volume": "18",
        "year": "2025"
    },
    "WuTong2024Rai3": {
        "abstract": "The emergence of 3D Gaussian splatting (3DGS) has greatly accelerated rendering in novel view synthesis. Unlike neural implicit representations like neural radiance fields (NeRFs) that represent a 3D scene with position and viewpoint-conditioned neural networks, 3D Gaussian splatting utilizes a set of Gaussian ellipsoids to model the scene so that efficient rendering can be accomplished by rasterizing Gaussian ellipsoids into images. Apart from fast rendering, the explicit representation of 3D Gaussian splatting also facilitates downstream tasks like dynamic reconstruction, geometry editing, and physical simulation. Considering the rapid changes and growing number of works in this field, we present a literature review of recent 3D Gaussian splatting methods, which can be roughly classified by functionality into 3D reconstruction, 3D editing, and other downstream applications. Traditional point-based rendering methods and the rendering formulation of 3D Gaussian splatting are also covered to aid understanding of this technique. This survey aims to help beginners to quickly get started in this field and to provide experienced researchers with a comprehensive overview, aiming to stimulate future development of the 3D Gaussian splatting representation.",
        "author": "Wu, Tong and Yuan, Yu-Jie and Zhang, Ling-Xiao and Yang, Jie and Cao, Yan-Pei and Yan, Ling-Qi and Gao, Lin",
        "doi": "10.1007/s41095-024-0436-y",
        "journal": "Computational Visual Media",
        "keywords": "type:Survey Paper and Foundational Work, topics: Artificial Intelligence, Cameras , Computer Graphics , Computer Science , Content creation , Editing , Ellipsoids , Geometry , Image Processing and Computer Vision , Image reconstruction , Neural networks , Physical simulation , Rendering , Representations , User Interfaces and Human Computer Interaction , User training",
        "number": "4",
        "pages": "613-642",
        "series": "Computational Visual Media",
        "title": "Recent advances in 3D Gaussian splatting",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/document/10897713",
        "volume": "10",
        "year": "2024"
    },
    "XuYukun2025A3Gf": {
        "abstract": "We present an animatable 3D Gaussian representation for synthesizing high-fidelity human videos under novel views and poses in real time. Given multi-view videos of a human subject, we learn a collection of 3D Gaussians in the canonical space of the rest pose. Each Gaussian is associated with a few basic properties (i.e., position, opacity, scale, rotation, spherical harmonics coefficients) representing the average human appearance across all video frames, as well as a latent code and a set of blend weights for dynamic appearance correction and pose transformation. The latent code is fed to an Multi-layer Perceptron (MLP) with a target pose to correct Gaussians in the canonical space to capture appearance changes under the target pose. The corrected Gaussians are then transformed to the target pose using linear blend skinning (LBS) with their blend weights. High-fidelity human images under novel views and poses can be rendered in real time through Gaussian splatting. Compared to state-of-the-art NeRF-based methods, our animatable Gaussian representation produces more compelling results with well captured details, and achieves superior rendering performance.",
        "address": "Beijing",
        "author": "Xu, Yukun and Ye, Keyang and Shao, Tianjia and Weng, Yanlin",
        "copyright": "Higher Education Press 2025",
        "doi": "10.1007/s11704-024-40497-5",
        "issn": "2095-2228",
        "journal": "Frontiers of Computer Science",
        "keywords": "type:Human Avatar Reconstruction,topics: Digital human, Approximation , Art techniques , CAD , Computer aided design , Computer Science , Methods , Multilayer perceptrons, Multilayers , Real time, Representations , Research Article, Rotating spheres , Spherical harmonics , Video",
        "language": "eng",
        "number": "9",
        "pages": "199704-",
        "publisher": "Higher Education Press",
        "series": "Frontiers of Computer Science",
        "title": "Animatable 3D Gaussians for modeling dynamic humans",
        "type": "article",
        "url": "https://link-springer-com.nottingham.idm.oclc.org/article/10.1007/s11704-024-40497-5",
        "volume": "19",
        "year": "2025"
    },
    "abdal2023gaussianshellmapsefficient": {
        "abstract": "Efficient generation of 3D digital humans is important in several industries, including virtual reality, social media, and cinematic production. 3D generative adversarial networks (GANs) have demonstrated state-of-the-art (SOTA) quality and diversity for generated assets. Current 3D GAN architectures, however, typically rely on volume representations, which are slow to render, thereby hampering the GAN training and requiring multi-view-inconsistent 2D upsamplers. Here, we introduce Gaussian Shell Maps (GSMs) as a framework that connects SOTA generator network architectures with emerging 3D Gaussian rendering primitives using an articulable multi shell--based scaffold. In this setting, a CNN generates a 3D texture stack with features that are mapped to the shells. The latter represent inflated and deflated versions of a template surface of a digital human in a canonical body pose. Instead of rasterizing the shells directly, we sample 3D Gaussians on the shells whose attributes are encoded in the texture features. These Gaussians are efficiently and differentiably rendered. The ability to articulate the shells is important during GAN training and, at inference time, to deform a body into arbitrary user-defined poses. Our efficient rendering scheme bypasses the need for view-inconsistent upsamplers and achieves high-quality multi-view consistent renderings at a native resolution of 512 \\times 512 pixels. We demonstrate that GSMs successfully generate 3D humans when trained on single-view datasets, including SHHQ and DeepFashion.",
        "archiveprefix": "arXiv",
        "author": "Rameen Abdal and Wang Yifan and Zifan Shi and Yinghao Xu and Ryan Po and Zhengfei Kuang and Qifeng Chen and Dit-Yan Yeung and Gordon Wetzstein",
        "doi": "10.48550/arXiv.2311.17857",
        "eprint": "2311.17857",
        "keywords": "type:Human Avatar Reconstruction, topics: Digital human",
        "primaryclass": "cs.CV",
        "series": "misc",
        "title": "Gaussian Shell Maps for Efficient 3D Human Generation",
        "type": "misc",
        "url": "https://arxiv.org/abs/2311.17857",
        "year": "2023"
    },
    "yang2023deformable3dgaussianshighfidelity": {
        "abstract": "Implicit neural representation has paved the way for new approaches to dynamic scene reconstruction. Nonetheless, cutting-edge dynamic neural rendering methods rely heavily on these implicit representations, which frequently struggle to capture the intricate details of objects in the scene. Furthermore, implicit methods have difficulty achieving real-time rendering in general dynamic scenes, limiting their use in a variety of tasks. To address the issues, we propose a deformable 3D Gaussians splatting method that reconstructs scenes using 3D Gaussians and learns them in canonical space with a deformation field to model monocular dynamic scenes. We also introduce an annealing smoothing training mechanism with no extra overhead, which can mitigate the impact of inaccurate poses on the smoothness of time interpolation tasks in real-world scenes. Through a differential Gaussian rasterizer, the deformable 3D Gaussians not only achieve higher rendering quality but also real-time rendering speed. Experiments show that our method outperforms existing methods significantly in terms of both rendering quality and speed, making it well-suited for tasks such as novel-view synthesis, time interpolation, and real-time rendering. Our code is available at https://github.com/ingra14m/Deformable-3D-Gaussians.",
        "author": "Yang, Ziyi and Gao, Xinyu and Zhou, Wen and Jiao, Shaohui and Zhang, Yuqing and Jin, Xiaogang",
        "booktitle": "2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
        "doi": "10.1109/CVPR52733.2024.01922",
        "keywords": "type:Monocular Dynamic Scene, topics:Scene Reconstruction, Training, Deformable models, Solid modeling,Interpolation,Three-dimensional displays,Annealing,Smoothing methods,3D Gaussian Splatting,Dynamic reconstruction",
        "pages": "20331-20341",
        "series": "misc",
        "title": "Deformable 3D Gaussians for High-Fidelity Monocular Dynamic Scene Reconstruction",
        "type": "misc",
        "url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Deformable_3D_Gaussians_for_High-Fidelity_Monocular_Dynamic_Scene_Reconstruction_CVPR_2024_paper.pdf",
        "year": "2024"
    }
}});